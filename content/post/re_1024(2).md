---
title: "Regression analysis: How do I interpret R-squared and assess the Goodness-of-Fit?"
date: 2021-10-24T12:05:35+02:00
description: "In this post, we’ll explore the R-squared (R2 ) statistic, some of its limitations, and uncover some surprises along the way. For instance, low R-squared values are not always bad and high R-squared values are not always good!"
categories:
- Research
- Statistics
tags:
- regression
- R-squared
- Goodness-of-Fit
keywords:
- tech
thumbnailImage: https://datascience696.files.wordpress.com/2016/08/regressioncurv.png
thumbnailImagePosition: left
metaAlignment: center
Math: True
---
Low R-squared values are not always bad and high R-squared values are not always good!
<!--more-->
After you have fit a linear model using regression analysis, ANOVA (Analysis of variance), or design of experiments (DOE), you need to determine how well the model fits the data. [In this post]https://blog.minitab.com/en/adventures-in-statistics-2/regression-analysis-how-do-i-interpret-r-squared-and-assess-the-goodness-of-fit), we’ll explore the R-squared (R2 ) statistic, some of its limitations, and uncover some surprises along the way. For instance, low R-squared values are not always bad and high R-squared values are not always good!
<!--more-->
{{< toc >}}
# What is Goodness-of-Fit for a linear model?

Linear regression calculates ***an equation that minimizes the distance between the fitted line and all of the data points***. Technically, ordinary least squares (OLS) regression minimizes the sum of the squared residuals.

In general, a model fits the data well if the **differences** between the observed values and the model's predicted values are **small** and **unbiased**.

Before you look at the statistical measures for goodness-of-fit, you should check the **residual plots** (Definition: Residual = Observed value - Fitted value). Residual plots can reveal unwanted residual patterns that indicate biased results more effectively than numbers. When your residual plots pass muster, you can trust your numerical results and check the goodness-of-fit statistics.

{{< hl-text orange >}}Roughly, Goodness-of-Fit indicates the discrepancy between observed values and the values expected under the model in question.{{< /hl-text >}} [^1]


[^1]: https://en.wikipedia.org/wiki/Goodness_of_fit

# What is R-squared?

R-squared is a statistical measure of how close the data are to the fitted regression line. It is also known as the coefficient of determination (可决系数), or the coefficient of multiple determination for multiple regression.

The definition of R-squared is fairly straight-forward; {{< hl-text orange >}}R-squared is the percentage of the response variable variation that is explained by a linear model.{{< /hl-text >}} Or:

**R-squared = Explained variation / Total variation**
Measured with two sums of squares formulas:
$$ R^2 = 1-\frac{SS_{res}}{SS_{tot}} $$

The sum of squares of residuals, also called the **residual sum of squares**:

$$ SS_{res} = \sum_i (y_i-f_i)^2 = \sum_i e_i^2$$

The **total sum of squares**[^squ] (proportional to the variance of the data):

$$ SS_{tot} = \sum_i (y_i-\bar{y})^2$$

Note: $\bar {y}$ is the mean of the observed data.

R-squared is always between 0 and 100%:
* 0% indicates that the model explains none of the variability of the response data around its mean.
* 100% indicates that the model explains all the variability of the response data around its mean.
![](https://upload.wikimedia.org/wikipedia/commons/thumb/8/86/Coefficient_of_Determination.svg/400px-Coefficient_of_Determination.svg.png)
$R^2 = 1- \frac{\color{blue}{SS_{res}}}{\color{red}{SS_{tot}}}$
The better the linear regression (on the right) fits the data in comparison to the simple average (on the left graph), the closer the value of $R^2$ is to 1. The areas of the blue squares represent the squared residuals with respect to the linear regression. The areas of the red squares represent the squared residuals with respect to the average value.

In general, the higher the R-squared, the better the model fits your data. However, there are important conditions for this guideline that I’ll talk about both in this post and my next post.

# Graphical representation of R-squared

Plotting fitted values by observed values graphically illustrates different R-squared values for regression models.
![](https://blog.minitab.com/hubfs/Imported_Blog_Media/fittedxobserved.gif)
The regression model on the left accounts for 38.0% of the variance while the one on the right accounts for 87.4%. The more variance that is accounted for by the regression model the closer the data points will fall to the fitted regression line. Theoretically, if a model could explain 100% of the variance, the fitted values would always equal the observed values and, therefore, all the data points would fall on the fitted regression line.

# Key limitations of R-squared

{{< hl-text orange>}}R-squared cannot determine whether the coefficient estimates and predictions are biased{{< /hl-text >}}, which is why you must assess the residual plots.

R-squared does not indicate whether a regression model is adequate. You can have a low R-squared value for a good model, or a high R-squared value for a model that does not fit the data!

[^squ]: For wide classes of linear models, the total sum of squares equals the explained sum of squares plus the residual sum of squares.
