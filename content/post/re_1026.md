---
title: "R-squared Shrinkage and Power and Sample Size Guidelines for Regression Analysis"
date: 2021-10-26T16:57:20+02:00
categories:
- Research
- Statistics
tags:
- regression
- R-squared
keywords:
- tech
thumbnailImage: https://blog.minitab.com/hubfs/Imported_Blog_Media/rsq_shrinkage_w640.png
thumbnailImagePosition: left
metaAlignment: center
Math: True
---
This blog looks at how to obtain an unbiased and reasonably precise estimate of the population R-squared. It also presents power and sample size guidelines for regression analysis.
<!--more-->
{{< toc >}}

Using a sample to estimate the properties of an entire population is common practice in statistics. For example, the mean from a random sample estimates that parameter for an entire population. In linear regression analysis, we’re used to the idea that the **regression coefficients** are estimates of the true parameters. However, it’s easy to forget that R-squared (R2) is also an estimate. Unfortunately, it has a problem that many other estimates don’t have. {{< hl-text orange>}}R-squared is inherently biased!{{< /hl-text >}}

# R-squared as a biased estimate

R-squared measures the strength of the relationship between the predictors and response. The R-squared in your regression output is a biased estimate based on your sample.
* An unbiased estimate[^unbiased] is one that is just as likely to be too high as it is to be too low, and it is correct on average. If you collect a random sample correctly, the sample mean is an unbiased estimate of the population mean.
* **A biased estimate** is **systematically too high or low**, and so is **the average**. ***It’s like a bathroom scale that always indicates you are heavier than you really are. No one wants that!***

R-squared is like the broken bathroom scale: it is deceptively large. Researchers have long recognized that regression’s optimization process takes advantage of **[chance correlations](http://www.vias.org/tmdatanaleng/cc_corr_bychance.html)** (偶然相关) in the sample data and inflates the R-squared.

This bias is a reason why some practitioners don’t use R-squared at all—it tends to be wrong.

# R-squared shrinkage

What should we do about this bias? Fortunately, there is a solution and you’re probably already familiar with it: adjusted R-squared. I’ve written about using the **[adjusted R-squared](https://blog.minitab.com/en/adventures-in-statistics-2/multiple-regession-analysis-use-adjusted-r-squared-and-predicted-r-squared-to-include-the-correct-number-of-variables)** to compare regression models with a different number of terms. Another use is that it is an unbiased estimator of the population R-squared.

Adjusted R-squared does what you’d do with that broken bathroom scale. If you knew the scale was consistently too high, you’d reduce it by an appropriate amount to produce an accurate weight. In statistics this is called shrinkage. (You Seinfeld fans are probably giggling now. Yes, George, we’re talking about shrinkage, but here it’s a good thing!)

We need to shrink the R-squared down so that it is not biased. Adjusted R-squared does this by comparing the sample size to the number of terms in your regression model.

Regression models that have many samples per term produce a better R-squared estimate and require less shrinkage. Conversely, models that have few samples per term require more shrinkage to correct the bias.

[^unbiased]: An unbiased estimator of a parameter is an estimator whose expected value is equal to the parameter. That is, if the estimator S is being used to estimate a parameter θ, then S is an unbiased estimator of θ if E(S)=θ. If an estimator S is unbiased, then on average it is equal to the number it is trying to estimate.
